{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EpiBERT Gradients and Gene Analysis Workflow\n",
        "# This notebook demonstrates a complete workflow for analyzing model gradients and gene predictions\n",
        "# using EpiBERT. It covers:\n",
        "# 1. Model gradient analysis for single genomic intervals\n",
        "# 2. Batch predictions for multiple genes\n",
        "# 3. Parsing and evaluating predictions\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Add parent directory to path for imports\n",
        "notebook_dir = Path(os.getcwd())\n",
        "sys.path.append(str(notebook_dir.parent))\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='tensorflow_addons')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# Core imports\n",
        "import tensorflow as tf\n",
        "import tensorflow.experimental.numpy as tnp\n",
        "from tensorflow import strings as tfs\n",
        "from tensorflow.keras import mixed_precision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.metrics import precision_recall_curve, auc, average_precision_score\n",
        "import kipoiseq\n",
        "\n",
        "import src.models.epibert_rampage_finetune as epibert\n",
        "import training_utils_rampage_finetune as training_utils\n",
        "import analysis.interval_and_plotting_utilities as utils\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU memory growth setup failed: {e}\")\n",
        "\n",
        "# Use mixed precision for better performance\n",
        "mixed_precision.set_global_policy('mixed_bfloat16')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - Update these paths for your environment\n",
        "# =======================================================\n",
        "\n",
        "# Model parameters\n",
        "SEQUENCE_LENGTH = 524288\n",
        "RESOLUTION = 4\n",
        "NUM_BINS = SEQUENCE_LENGTH // RESOLUTION\n",
        "OUTPUT_LENGTH = NUM_BINS // 32\n",
        "CROP_SIZE = 1600\n",
        "MASK_INDICES = '2041-2053'\n",
        "\n",
        "# File paths - update these for your setup\n",
        "FASTA_FILE = '/home/jupyter/reference/hg38_erccpatch.fa'\n",
        "ATAC_FILE = \"/home/jupyter/datasets/ATAC/HG_K562.bed.gz\" # see data processing notebook\n",
        "RNA_FILE = \"/home/jupyter/datasets/ATAC/HG_K562.rampage.bed.gz\" # bed formatted RAMPAGE file\n",
        "TF_FILE = '/home/jupyter/datasets/ATAC/HG_K562.tsv'\n",
        "ENHANCER_FILE = '/home/jupyter/datasets/eg/hg38_eg.bed'\n",
        "\n",
        "# Model checkpoint path\n",
        "CHECKPOINT_PATH = \"gs://genformer_europe_west_copy/524k/rampage_finetune/models/genformer_524k_LR1-5.0e-04_LR2-5.0e-04_C-512_640_640_768_896_1024_T-8_motif-True_9_m7o9qhwt/ckpt-16\"\n",
        "\n",
        "# Output directories\n",
        "TEMP_DIR = \"temp_files\"\n",
        "OUTPUT_DIR = \"output_gradients\"\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "os.makedirs(TEMP_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Gene dictionary for analysis\n",
        "GENES_DICT = {\n",
        "    \"HNRNPA1\": \"chr12:54279939-54281439\",\n",
        "    \"NFE2\": \"chr12:54300287-54301787\", \n",
        "    \"COPZ1\": \"chr12:54324339-54325839\",\n",
        "    \"ITGA5\": \"chr12:54418516-54420016\",\n",
        "    \"WDR83OS\": \"chr19:12668901-12670401\",\n",
        "    \"DHPS\": \"chr19:12681137-12682637\",\n",
        "    \"C19orf43\": \"chr19:12734025-12735525\",\n",
        "    \"JUNB\": \"chr19:12790745-12792245\",\n",
        "    \"PRDX2\": \"chr19:12801160-12802660\",\n",
        "    \"RNASEH2A\": \"chr19:12805863-12807363\",\n",
        "    \"MYC\": \"chr8:127735318-127736818\",\n",
        "    \"GATA1\": \"chrX:48785823-48787323\"\n",
        "}\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "SEED = 6\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Model and Data Extractors\n",
        "\n",
        "#  FASTA extractor\n",
        "fasta_extractor = utils.FastaStringExtractor(FASTA_FILE)\n",
        "\n",
        "# Device strategy - use GPU since model is too large to run on CPU \n",
        "device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
        "\n",
        "# Initialize model with GPU strategy\n",
        "\n",
        "with tf.device(device):\n",
        "    model = epibert.epibert_rampage_finetune(\n",
        "        kernel_transformation='relu_kernel_transformation',\n",
        "        dropout_rate=0.20,\n",
        "        pointwise_dropout_rate=0.10,\n",
        "        input_length=SEQUENCE_LENGTH,\n",
        "        output_length=4096,\n",
        "        final_output_length=896,\n",
        "        num_heads=8,\n",
        "        numerical_stabilizer=0.0000001,\n",
        "        max_seq_length=4096,\n",
        "        seed=SEED,\n",
        "        norm=True,\n",
        "        BN_momentum=0.90,\n",
        "        normalize=True,\n",
        "        use_rot_emb=True,\n",
        "        num_transformer_layers=8,\n",
        "        final_point_scale=6,\n",
        "        filter_list_seq=[512, 640, 640, 768, 896, 1024],\n",
        "        filter_list_atac=[32, 64],\n",
        "        predict_atac=True\n",
        "    )\n",
        "    \n",
        "    model.load_weights(CHECKPOINT_PATH)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Part 1: Single Interval Gradient Analysis\n",
        "\n",
        "This section demonstrates how to analyze model gradients for a single genomic interval to understand which regions are important for a models predictions\n",
        "1. Extract inputs for a specific genomic region\n",
        "2. Compute gradients using integrated gradients\n",
        "3. Visualize results showing predictions, gradients, and enhancer annotations\n",
        "4. Export tracks for further analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single Interval Analysis - HNRNPA1 Example\n",
        "\n",
        "# Select a specific genomic interval for detailed analysis\n",
        "example_gene = \"HNRNPA1\"\n",
        "interval = GENES_DICT[example_gene]\n",
        "\n",
        "print(f\"Analyzing interval: {example_gene} at {interval}\")\n",
        "\n",
        "# Extract inputs for the genomic interval\n",
        "print(\"Extracting inputs for interval analysis...\")\n",
        "inputs, masked_atac, target_atac, target_atac_uncropped, rna_arr, masked_atac_reshape, mask, mask_centered = \\\n",
        "    utils.return_all_inputs_simple(\n",
        "        interval, \n",
        "        ATAC_FILE, \n",
        "        RNA_FILE, \n",
        "        SEQUENCE_LENGTH,\n",
        "        NUM_BINS, \n",
        "        RESOLUTION, \n",
        "        TF_FILE, \n",
        "        CROP_SIZE, \n",
        "        OUTPUT_LENGTH,\n",
        "        fasta_extractor, \n",
        "        MASK_INDICES, \n",
        "        None  # No strategy needed for single GPU\n",
        "    )\n",
        "\n",
        "print(f\" Inputs extracted:\")\n",
        "print(f\"  - Sequence shape: {inputs[0].shape}\")\n",
        "print(f\"  - ATAC shape: {masked_atac.shape}\")\n",
        "print(f\"  - RNA shape: {rna_arr.shape}\")\n",
        "print(f\"  - Target ATAC shape: {target_atac.shape}\")\n",
        "\n",
        "# Compute gradients using integrated gradients\n",
        "print(\"Computing gradients...\")\n",
        "with tf.device(device):\n",
        "    seq, seq_grads, atac_grads, prediction, att_matrices, att_matrices_norm = \\\n",
        "        model.contribution_input_grad_dist_simple(inputs, mask)\n",
        "\n",
        "print(f\"✓ Gradients computed:\")\n",
        "print(f\"  - Sequence gradients shape: {seq_grads.shape}\")\n",
        "print(f\"  - ATAC gradients shape: {atac_grads.shape}\")\n",
        "print(f\"  - Prediction shape: {prediction.shape}\")\n",
        "print(f\"  - Attention matrices shape: {att_matrices.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Predictions vs Ground Truth\n",
        "\n",
        "print(\"Creating prediction comparison plot...\")\n",
        "\n",
        "# Plot predicted vs actual RNA expression\n",
        "tracks = {\n",
        "    'actual_rna': (rna_arr[:, 0], 'red'),\n",
        "    'predicted_rna': (prediction[0, :, 0], 'blue')\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "utils.plot_tracks(tracks, 0, 896, 25)\n",
        "plt.title(f'{example_gene} - RNA Expression: Predicted vs Actual')\n",
        "plt.xlabel('Genomic Position (bins)')\n",
        "plt.ylabel('Expression Level')\n",
        "plt.legend(['Actual RNA', 'Predicted RNA'])\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate correlation between predicted and actual\n",
        "correlation = pearsonr(rna_arr[:, 0].numpy(), prediction[0, :, 0].numpy())[0]\n",
        "print(f\"Pearson correlation (predicted vs actual): {correlation:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process and Visualize Gradients\n",
        "\n",
        "print(\"Processing gradients and enhancer annotations...\")\n",
        "\n",
        "# Process ATAC gradients\n",
        "grad_input = tf.abs(atac_grads[0][:, 0]) * masked_atac[:, 0]\n",
        "reshaped_grad = tf.reduce_sum(tf.reshape(grad_input, [4096, 32]), axis=1)\n",
        "\n",
        "# Process sequence gradients\n",
        "seq_grad_input = tf.reduce_sum(\n",
        "    tf.reshape(\n",
        "        tf.reduce_sum(tf.abs(seq_grads) * seq[0][0, :, :], axis=1),\n",
        "        [-1, 128]\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Load enhancer annotations if available\n",
        "try:\n",
        "    enhancer_file = f'/home/jupyter/datasets/eg/hg38_eg.{example_gene}.bed.gz'\n",
        "    eg = utils.return_eg(interval, enhancer_file, SEQUENCE_LENGTH)\n",
        "    eg_grouped = tf.reduce_max(tf.reshape(eg, [4096, 128]), axis=1)\n",
        "    \n",
        "    # Load significance annotations\n",
        "    sig_file = f'/home/jupyter/datasets/eg/hg38_eg.{example_gene}.sig.bed.gz'\n",
        "    eg_sig = utils.return_eg(interval, sig_file, SEQUENCE_LENGTH)\n",
        "    eg_grouped_sig = tf.reduce_max(tf.reshape(eg_sig, [4096, 128]), axis=1)\n",
        "    \n",
        "    enhancer_available = True\n",
        "    print(\"✓ Enhancer annotations loaded\")\n",
        "except:\n",
        "    enhancer_available = False\n",
        "    print(\"! Enhancer annotations not available\")\n",
        "\n",
        "# Normalize gradients for visualization\n",
        "atac_grads_norm = reshaped_grad / tf.reduce_max(reshaped_grad)\n",
        "seq_grads_norm = seq_grad_input / tf.reduce_max(seq_grad_input)\n",
        "atac_signal_norm = target_atac_uncropped[:, 0] / tf.reduce_max(target_atac_uncropped[:, 0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Comprehensive Gradient Visualization\n",
        "# ==========================================\n",
        "\n",
        "tracks = {\n",
        "    'atac_gradients': (atac_grads_norm, 'blue'),\n",
        "    'sequence_gradients': (seq_grads_norm, 'green'),\n",
        "    'atac_signal': (atac_signal_norm, 'orange'),\n",
        "    'combined_gradients': (atac_grads_norm + seq_grads_norm, 'purple')\n",
        "}\n",
        "\n",
        "# Add enhancer tracks if available\n",
        "if enhancer_available:\n",
        "    tracks['enhancers'] = (eg_grouped, 'red')\n",
        "    tracks['significant_enhancers'] = (eg_grouped_sig, 'pink')\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(20, 12))\n",
        "utils.plot_tracks(tracks, 0, 4096, 1.0)\n",
        "plt.title(f'{example_gene} - Gradient Analysis Across Genomic Region')\n",
        "plt.xlabel('Genomic Position (128bp bins)')\n",
        "plt.ylabel('Normalized Signal')\n",
        "\n",
        "# Create legend\n",
        "legend_labels = [\n",
        "    'ATAC Gradients',\n",
        "    'Sequence Gradients', \n",
        "    'ATAC Signal',\n",
        "    'Combined Gradients'\n",
        "]\n",
        "if enhancer_available:\n",
        "    legend_labels.extend(['Enhancers', 'Significant Enhancers'])\n",
        "\n",
        "plt.legend(legend_labels, loc='upper right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Gradient visualization completed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Part 2: Batch Gene Analysis\n",
        "\n",
        "This section demonstrates how to run gradient analysis across multiple genes systematically. We'll:\n",
        "\n",
        "1. Process enhancer files for each gene\n",
        "2. Compute gradients for all genes in batch\n",
        "3. Extract enhancer-specific scores \n",
        "4. Save results for downstream analysis\n",
        "\n",
        "This is useful for large-scale enhancer-gene interaction studies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch Processing: Prepare Enhancer Files\n",
        "\n",
        "print(\"Preparing enhancer files for batch analysis...\")\n",
        "\n",
        "def prepare_enhancer_files(gene_name, enhancer_file, temp_dir):\n",
        "    \"\"\"Prepare enhancer files for a specific gene\"\"\"\n",
        "    \n",
        "    # Extract enhancers for this gene\n",
        "    gene_bed_file = f\"{temp_dir}/{gene_name}.eg.bed\"\n",
        "    command = f\"grep '{gene_name}' {enhancer_file} | sort -k1,1 -k2,2n > {gene_bed_file}\"\n",
        "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "    \n",
        "    if result.returncode != 0:\n",
        "        print(f\"Warning: No enhancers found for {gene_name}\")\n",
        "        return None\n",
        "    \n",
        "    # Add encoding column\n",
        "    encoded_file = f\"{temp_dir}/{gene_name}.eg.encoded.bed\"\n",
        "    command = f\"awk '{{OFS=\\\"\\\\t\\\"}}{{print $1,$2,$3,NR}}' {gene_bed_file} > {encoded_file}\"\n",
        "    subprocess.run(command, shell=True)\n",
        "    \n",
        "    # Compress and index\n",
        "    command = f\"bgzip -f {encoded_file}\"\n",
        "    subprocess.run(command, shell=True)\n",
        "    \n",
        "    command = f\"tabix -f {encoded_file}.gz\"\n",
        "    subprocess.run(command, shell=True)\n",
        "    \n",
        "    return f\"{encoded_file}.gz\"\n",
        "\n",
        "# Process enhancer files for all genes\n",
        "enhancer_files = {}\n",
        "for gene_name in GENES_DICT.keys():\n",
        "    print(f\"  Processing enhancers for {gene_name}...\")\n",
        "    enhancer_file = prepare_enhancer_files(gene_name, ENHANCER_FILE, TEMP_DIR)\n",
        "    if enhancer_file:\n",
        "        enhancer_files[gene_name] = enhancer_file\n",
        "\n",
        "print(f\"✓ Prepared enhancer files for {len(enhancer_files)} genes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch Processing: Compute Gradients for All Genes\n",
        "print(\"Starting batch gradient computation...\")\n",
        "\n",
        "def process_gene_gradients(gene_name, interval, enhancer_file):\n",
        "    \"\"\"Process gradients for a single gene\"\"\"\n",
        "    \n",
        "    print(f\"  Processing {gene_name}...\")\n",
        "    \n",
        "    # Extract inputs\n",
        "    inputs, masked_atac, target_atac, target_atac_uncropped, rna_arr, \\\n",
        "    masked_atac_reshape, mask, mask_centered = utils.return_all_inputs_simple(\n",
        "        interval, ATAC_FILE, RNA_FILE, SEQUENCE_LENGTH, NUM_BINS, RESOLUTION,\n",
        "        TF_FILE, CROP_SIZE, OUTPUT_LENGTH, fasta_extractor, MASK_INDICES, None\n",
        "    )\n",
        "    \n",
        "    # Compute gradients\n",
        "    with tf.device(device):\n",
        "        seq, seq_grads, atac_grads, prediction, att_matrices, att_matrices_norm = \\\n",
        "            model.contribution_input_grad_dist_simple(inputs, mask)\n",
        "    \n",
        "    # Process ATAC gradients\n",
        "    grad_input = tf.abs(atac_grads[0][:, 0]) * masked_atac[:, 0]\n",
        "    reshaped_grad = tf.reduce_sum(tf.reshape(grad_input, [4096, 32]), axis=1)\n",
        "    \n",
        "    # Load enhancer annotations\n",
        "    eg = utils.return_eg(interval, enhancer_file, SEQUENCE_LENGTH)\n",
        "    eg_grouped = tf.reduce_max(tf.reshape(eg, [4096, 128]), axis=1)\n",
        "    eg_grouped = eg_grouped.numpy()\n",
        "    eg_unique = np.unique(eg_grouped)\n",
        "    \n",
        "    # Scale gradients\n",
        "    atac_grads_scaled = reshaped_grad / tf.reduce_max(reshaped_grad)\n",
        "    \n",
        "    # Extract enhancer-specific gradient scores\n",
        "    enhancer_scores = []\n",
        "    for enhancer_id in eg_unique[eg_unique != 0]:\n",
        "        indices = np.where(eg_grouped == enhancer_id)[0]\n",
        "        gradient_score = tf.reduce_sum(tf.gather(atac_grads_scaled, indices)).numpy()\n",
        "        enhancer_scores.append({\n",
        "            'encoding': int(enhancer_id),\n",
        "            'grad_out': gradient_score\n",
        "        })\n",
        "    \n",
        "    # Save results\n",
        "    output_file = f\"{OUTPUT_DIR}/{gene_name}.eg.preds.tsv\"\n",
        "    df = pd.DataFrame(enhancer_scores)\n",
        "    df.to_csv(output_file, sep='\\t', index=False)\n",
        "    \n",
        "    return len(enhancer_scores)\n",
        "\n",
        "# Process all genes\n",
        "total_enhancers = 0\n",
        "processing_results = {}\n",
        "\n",
        "for gene_name, interval in GENES_DICT.items():\n",
        "    if gene_name in enhancer_files:\n",
        "        try:\n",
        "            n_enhancers = process_gene_gradients(gene_name, interval, enhancer_files[gene_name])\n",
        "            processing_results[gene_name] = n_enhancers\n",
        "            total_enhancers += n_enhancers\n",
        "            print(f\"    ✓ {gene_name}: {n_enhancers} enhancers processed\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ✗ {gene_name}: Error - {str(e)}\")\n",
        "            processing_results[gene_name] = 0\n",
        "    else:\n",
        "        print(f\"    ✗ {gene_name}: No enhancer file available\")\n",
        "        processing_results[gene_name] = 0\n",
        "\n",
        "print(f\"\\n✓ Batch processing completed:\")\n",
        "print(f\"  - Genes processed: {len([g for g, n in processing_results.items() if n > 0])}\")\n",
        "print(f\"  - Total enhancers: {total_enhancers}\")\n",
        "print(f\"  - Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Part 3: Parse and Evaluate Predictions\n",
        "\n",
        "This section demonstrates how to parse the gradient predictions and evaluate their performance. We'll:\n",
        "\n",
        "1. **Load and merge** prediction results with enhancer annotations\n",
        "2. **Filter data** by distance and other criteria\n",
        "3. **Evaluate performance** using precision-recall curves\n",
        "4. **Compare** against baseline methods (e.g., ABC scores)\n",
        "\n",
        "This helps assess how well the model's gradients identify functional enhancers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and Parse Prediction Results\n",
        "# =================================\n",
        "\n",
        "print(\"Loading and parsing prediction results...\")\n",
        "\n",
        "def load_and_parse_gene_results(gene_name, temp_dir, output_dir):\n",
        "    \"\"\"Load predictions and merge with enhancer annotations\"\"\"\n",
        "    \n",
        "    # Load predictions\n",
        "    pred_file = f\"{output_dir}/{gene_name}.eg.preds.tsv\"\n",
        "    if not os.path.exists(pred_file):\n",
        "        return None\n",
        "    \n",
        "    preds = pd.read_csv(pred_file, sep='\\t')\n",
        "    \n",
        "    # Load enhancer annotations\n",
        "    enhancer_file = f\"{temp_dir}/{gene_name}.eg.bed\"\n",
        "    if not os.path.exists(enhancer_file):\n",
        "        return None\n",
        "    \n",
        "    enhancer_df = pd.read_csv(enhancer_file, sep='\\t', header=None)\n",
        "    enhancer_df.columns = ['chrom', 'start', 'stop', 'info', 'blank']\n",
        "    \n",
        "    # Parse enhancer information\n",
        "    enhancer_df[['gene_name', 'true', 'abc_score', 'distance']] = \\\n",
        "        enhancer_df['info'].str.split('_', expand=True)\n",
        "    \n",
        "    # Add encoding column\n",
        "    enhancer_df['encoding'] = enhancer_df.reset_index().index + 1\n",
        "    \n",
        "    # Merge predictions with annotations\n",
        "    merged_df = enhancer_df.merge(preds, on='encoding', how='inner')\n",
        "    \n",
        "    # Clean up data types\n",
        "    merged_df['true'] = merged_df['true'].replace({'TRUE': 1, 'FALSE': 0})\n",
        "    merged_df['abc_score'] = merged_df['abc_score'].astype(float)\n",
        "    merged_df['distance'] = merged_df['distance'].astype(int)\n",
        "    \n",
        "    return merged_df\n",
        "\n",
        "# Load results for all genes\n",
        "all_results = []\n",
        "for gene_name in GENES_DICT.keys():\n",
        "    if processing_results.get(gene_name, 0) > 0:\n",
        "        gene_results = load_and_parse_gene_results(gene_name, TEMP_DIR, OUTPUT_DIR)\n",
        "        if gene_results is not None:\n",
        "            all_results.append(gene_results)\n",
        "            print(f\"  ✓ {gene_name}: {len(gene_results)} enhancers loaded\")\n",
        "\n",
        "# Combine all results\n",
        "if all_results:\n",
        "    combined_df = pd.concat(all_results, ignore_index=True)\n",
        "    print(f\"\\n✓ Combined dataset created:\")\n",
        "    print(f\"  - Total enhancers: {len(combined_df)}\")\n",
        "    print(f\"  - Genes represented: {combined_df['gene_name'].nunique()}\")\n",
        "    print(f\"  - Positive examples: {combined_df['true'].sum()}\")\n",
        "    print(f\"  - Positive rate: {combined_df['true'].mean():.3f}\")\n",
        "else:\n",
        "    print(\"✗ No results to combine\")\n",
        "    combined_df = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter Data and Evaluate Performance\n",
        "# ====================================\n",
        "\n",
        "if combined_df is not None:\n",
        "    print(\"Filtering data and evaluating performance...\")\n",
        "    \n",
        "    # Apply distance filter (common in enhancer studies)\n",
        "    max_distance = 100000  # 100kb\n",
        "    min_distance = 1000    # 1kb\n",
        "    \n",
        "    filtered_df = combined_df[\n",
        "        (combined_df['distance'] <= max_distance) & \n",
        "        (combined_df['distance'] >= min_distance)\n",
        "    ].copy()\n",
        "    \n",
        "    print(f\"✓ Applied distance filter ({min_distance}-{max_distance} bp):\")\n",
        "    print(f\"  - Filtered enhancers: {len(filtered_df)}\")\n",
        "    print(f\"  - Positive examples: {filtered_df['true'].sum()}\")\n",
        "    print(f\"  - Positive rate: {filtered_df['true'].mean():.3f}\")\n",
        "    \n",
        "    # Evaluate EpiBERT gradients\n",
        "    if len(filtered_df) > 0 and filtered_df['true'].sum() > 0:\n",
        "        precision_grad, recall_grad, _ = precision_recall_curve(\n",
        "            filtered_df['true'], filtered_df['grad_out']\n",
        "        )\n",
        "        auprc_grad = auc(recall_grad, precision_grad)\n",
        "        ap_grad = average_precision_score(filtered_df['true'], filtered_df['grad_out'])\n",
        "        \n",
        "        print(f\"\\n✓ EpiBERT Gradients Performance:\")\n",
        "        print(f\"  - AUPRC: {auprc_grad:.4f}\")\n",
        "        print(f\"  - Average Precision: {ap_grad:.4f}\")\n",
        "        \n",
        "        # Evaluate ABC scores for comparison\n",
        "        precision_abc, recall_abc, _ = precision_recall_curve(\n",
        "            filtered_df['true'], filtered_df['abc_score']\n",
        "        )\n",
        "        auprc_abc = auc(recall_abc, precision_abc)\n",
        "        ap_abc = average_precision_score(filtered_df['true'], filtered_df['abc_score'])\n",
        "        \n",
        "        print(f\"\\n✓ ABC Scores Performance:\")\n",
        "        print(f\"  - AUPRC: {auprc_abc:.4f}\")\n",
        "        print(f\"  - Average Precision: {ap_abc:.4f}\")\n",
        "        \n",
        "        # Create comparison plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(recall_grad, precision_grad, label=f'EpiBERT Gradients (AP={ap_grad:.3f})', linewidth=2)\n",
        "        plt.plot(recall_abc, precision_abc, label=f'ABC Scores (AP={ap_abc:.3f})', linewidth=2)\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title('Precision-Recall Curves')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.hist(filtered_df[filtered_df['true'] == 1]['grad_out'], \n",
        "                alpha=0.7, label='True Enhancers', bins=30, density=True)\n",
        "        plt.hist(filtered_df[filtered_df['true'] == 0]['grad_out'], \n",
        "                alpha=0.7, label='Non-Enhancers', bins=30, density=True)\n",
        "        plt.xlabel('EpiBERT Gradient Score')\n",
        "        plt.ylabel('Density')\n",
        "        plt.title('Score Distribution')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Performance comparison\n",
        "        print(f\"\\n✓ Performance Comparison:\")\n",
        "        print(f\"  - EpiBERT improvement over ABC: {((ap_grad - ap_abc) / ap_abc * 100):+.1f}%\")\n",
        "        \n",
        "    else:\n",
        "        print(\"✗ Insufficient data for evaluation\")\n",
        "        \n",
        "else:\n",
        "    print(\"✗ No combined dataset available for evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export Results and Summary\n",
        "# ==========================\n",
        "\n",
        "if combined_df is not None:\n",
        "    print(\"Exporting results and creating summary...\")\n",
        "    \n",
        "    # Save combined results\n",
        "    output_file = f\"{OUTPUT_DIR}/combined_results.tsv\"\n",
        "    filtered_df.to_csv(output_file, sep='\\t', index=False)\n",
        "    print(f\"✓ Combined results saved: {output_file}\")\n",
        "    \n",
        "    # Create summary statistics\n",
        "    summary_stats = {\n",
        "        'total_genes': len(GENES_DICT),\n",
        "        'processed_genes': len([g for g, n in processing_results.items() if n > 0]),\n",
        "        'total_enhancers': len(filtered_df),\n",
        "        'positive_enhancers': filtered_df['true'].sum(),\n",
        "        'positive_rate': filtered_df['true'].mean(),\n",
        "        'epibert_ap': ap_grad if 'ap_grad' in locals() else None,\n",
        "        'abc_ap': ap_abc if 'ap_abc' in locals() else None,\n",
        "        'distance_filter': f\"{min_distance}-{max_distance}bp\"\n",
        "    }\n",
        "    \n",
        "    # Save summary\n",
        "    summary_file = f\"{OUTPUT_DIR}/analysis_summary.json\"\n",
        "    import json\n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(summary_stats, f, indent=2)\n",
        "    print(f\"✓ Analysis summary saved: {summary_file}\")\n",
        "    \n",
        "    # Display final summary\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"WORKFLOW SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Genes analyzed: {summary_stats['processed_genes']}/{summary_stats['total_genes']}\")\n",
        "    print(f\"Enhancers evaluated: {summary_stats['total_enhancers']}\")\n",
        "    print(f\"Positive examples: {summary_stats['positive_enhancers']} ({summary_stats['positive_rate']:.1%})\")\n",
        "    \n",
        "    if summary_stats['epibert_ap'] is not None:\n",
        "        print(f\"EpiBERT AP: {summary_stats['epibert_ap']:.4f}\")\n",
        "        print(f\"ABC AP: {summary_stats['abc_ap']:.4f}\")\n",
        "        improvement = ((summary_stats['epibert_ap'] - summary_stats['abc_ap']) / summary_stats['abc_ap'] * 100)\n",
        "        print(f\"Improvement: {improvement:+.1f}%\")\n",
        "    \n",
        "    print(f\"Distance filter: {summary_stats['distance_filter']}\")\n",
        "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "else:\n",
        "    print(\"✗ No results to export\")\n",
        "\n",
        "print(\"\\n✓ Workflow completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates a complete workflow for EpiBERT gradient analysis and enhancer prediction:\n",
        "\n",
        "### Part 1: Single Interval Analysis\n",
        "- **Model gradients** computed using integrated gradients\n",
        "- **Visualization** of gradients, predictions, and enhancer annotations\n",
        "- **Correlation analysis** between predicted and actual RNA expression\n",
        "\n",
        "### Part 2: Batch Gene Analysis  \n",
        "- **Systematic processing** of multiple genes\n",
        "- **Enhancer file preparation** and indexing\n",
        "- **Gradient computation** for all enhancers across genes\n",
        "- **Results export** for downstream analysis\n",
        "\n",
        "### Part 3: Prediction Evaluation\n",
        "- **Data parsing** and merging with enhancer annotations\n",
        "- **Performance evaluation** using precision-recall curves\n",
        "- **Comparison** with baseline methods (ABC scores)\n",
        "- **Statistical analysis** and result export\n",
        "\n",
        "### Key Outputs\n",
        "\n",
        "1. **Individual gene results**: `{OUTPUT_DIR}/{gene}.eg.preds.tsv`\n",
        "2. **Combined dataset**: `{OUTPUT_DIR}/combined_results.tsv`\n",
        "3. **Analysis summary**: `{OUTPUT_DIR}/analysis_summary.json`\n",
        "4. **Visualizations**: Gradient plots and performance curves\n",
        "\n",
        "### Usage Notes\n",
        "\n",
        "- **GPU acceleration**: Automatically uses GPU if available\n",
        "- **Memory management**: Configured to prevent OOM errors\n",
        "- **Modular design**: Each section can be run independently\n",
        "- **Error handling**: Graceful handling of missing files/data\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Scale up**: Process larger gene sets or genome-wide enhancers\n",
        "2. **Compare models**: Test different EpiBERT variants or architectures  \n",
        "3. **Integrate data**: Combine with ChIP-seq, Hi-C, or other genomic data\n",
        "4. **Validate predictions**: Test top-scoring enhancers experimentally\n",
        "\n",
        "### Requirements\n",
        "\n",
        "- EpiBERT model checkpoint\n",
        "- Reference genome FASTA file\n",
        "- ATAC-seq and RNA-seq data files\n",
        "- Enhancer annotation files\n",
        "- Sufficient GPU memory for gradient computation\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
